{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75be9d8-3aca-43ca-aca3-234a9d685a38",
   "metadata": {},
   "source": [
    "**Task 4 - CodingAlpha Internship Programme**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc593109-3d85-4a11-96c5-1a935f68e274",
   "metadata": {},
   "source": [
    "**Topic Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087dfbf-8e80-48be-914d-a74cf7bfb4df",
   "metadata": {},
   "source": [
    "**Introduction to Preprocessing Task**\n",
    "\n",
    "Data preprocessing is a crucial step in the data science workflow that involves preparing raw data for analysis by handling missing values, outliers, and ensuring the data is in the right format for modeling. The goal is to improve the quality of the data, making it suitable for building robust and reliable machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a38ae-7ce5-4d8b-b6a8-b8d8dd3fd1e9",
   "metadata": {},
   "source": [
    "**Why the Titanic Dataset?**\n",
    "\n",
    "The Titanic dataset is a popular dataset in data science and machine learning due to its simplicity and the variety of preprocessing challenges it presents. It includes:  \n",
    "\n",
    "1. Categorical Features: Such as 'sex', 'embarked', and 'class', which need to be converted into numerical format.\n",
    "2. Numerical Features: Such as 'age' and 'fare', which may contain missing values and outliers.\n",
    "3. Missing Values: Some records have missing values that need to be handled appropriately.\n",
    "4. Outliers: The dataset contains outliers that can skew analysis and model performance.\n",
    "5. Target Variable: The 'survived' column makes it ideal for classification problems.\n",
    "\n",
    "These characteristics make the Titanic dataset are best choice for demonstrating data preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a3c29-7f60-4c33-a079-cd5cc5f2938d",
   "metadata": {},
   "source": [
    "**Data Preprocessing Steps**\n",
    "\n",
    "1. Loading the Dataset:\n",
    "\n",
    "The dataset is loaded into a pandas DataFrame to begin the preprocessing tasks.\n",
    "\n",
    "2. Handling Missing Values:\n",
    "\n",
    "Numerical Features: The 'age' column is imputed with the mean value.\n",
    "Categorical Features: The 'embarked' column is imputed with the most frequent value.\n",
    "\n",
    "3. Detecting and Handling Outliers:\n",
    "\n",
    "Outliers are identified using the Interquartile Range (IQR) method and removed from the dataset.\n",
    "\n",
    "4. Normalizing/Scaling Features:\n",
    "\n",
    "Numerical features such as 'age' and 'fare' are standardized using StandardScaler to ensure they have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "5. Converting Categorical Variables:\n",
    "\n",
    "Categorical variables are converted into dummy/indicator variables using one-hot encoding to prepare them for machine learning models.\n",
    "\n",
    "6. Splitting the Dataset:\n",
    "\n",
    "The dataset is split into training and testing sets to evaluate the model's performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f7865-dc15-4407-87b9-0d00cdbe0052",
   "metadata": {},
   "source": [
    "**Implementing the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b35c6-de76-4718-a352-75ecd0e19bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56eff4f8-5763-4231-80ac-743ee69c29bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "1  woman       False    C    Cherbourg   yes  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Dataset \n",
    "df = sns.load_dataset('titanic')\n",
    "print(\"Original Dataset:\")\n",
    "print(df.head())\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b43ec1f0-df5b-4959-a068-052250bc3da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after handling missing values:\n",
      "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "1  woman       False    C    Cherbourg   yes  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n",
      "Missing values after imputation:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age              0\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         0\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handling missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['embarked'] = imputer.fit_transform(df[['embarked']])\n",
    "\n",
    "print(\"Dataset after handling missing values:\")\n",
    "print(df.head())\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4873dc02-1a20-4dd3-83af-96b6bb4a8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns for outlier detection\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Detect and handle outliers using IQR for numerical columns only\n",
    "Q1 = df[numerical_cols].quantile(0.25)\n",
    "Q3 = df[numerical_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4c813a3-4313-4496-8a9b-cb0a6e6f1900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing outliers:\n",
      "   survived  pclass     sex        age  sibsp  parch     fare embarked  class  \\\n",
      "0         0       3    male  22.000000      1      0   7.2500        S  Third   \n",
      "2         1       3  female  26.000000      0      0   7.9250        S  Third   \n",
      "3         1       1  female  35.000000      1      0  53.1000        S  First   \n",
      "4         0       3    male  35.000000      0      0   8.0500        S  Third   \n",
      "5         0       3    male  29.699118      0      0   8.4583        Q  Third   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n",
      "5    man        True  NaN   Queenstown    no   True  \n",
      "Number of rows after removing outliers: 577\n"
     ]
    }
   ],
   "source": [
    "# Filtering out the outliers using conditions for numerical columns only\n",
    "outlier_condition = ~((df[numerical_cols] < (Q1 - 1.5 * IQR)) | (df[numerical_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "df_filtered = df[outlier_condition]\n",
    "\n",
    "print(\"Dataset after removing outliers:\")\n",
    "print(df_filtered.head())\n",
    "print(f\"Number of rows after removing outliers: {df_filtered.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b60c9733-f323-4056-9f1d-bdc26c526178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after normalization/scaling:\n",
      "        age      fare\n",
      "0 -0.909802 -0.609448\n",
      "2 -0.439745 -0.555858\n",
      "3  0.617882  3.030715\n",
      "4  0.617882 -0.545934\n",
      "5 -0.005046 -0.513517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prath\\AppData\\Local\\Temp\\ipykernel_3004\\927346272.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[['age', 'fare']] = scaler.fit_transform(df_filtered[['age', 'fare']])\n"
     ]
    }
   ],
   "source": [
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "df_filtered[['age', 'fare']] = scaler.fit_transform(df_filtered[['age', 'fare']])\n",
    "\n",
    "print(\"Dataset after normalization/scaling:\")\n",
    "print(df_filtered[['age', 'fare']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35ff50d0-fe23-4a33-b282-97b447821258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after converting categorical variables to dummy variables:\n",
      "   pclass       age  sibsp  parch      fare  adult_male  alone  sex_male  \\\n",
      "0       3 -0.909802      1      0 -0.609448        True  False         1   \n",
      "2       3 -0.439745      0      0 -0.555858       False   True         0   \n",
      "3       1  0.617882      1      0  3.030715       False  False         0   \n",
      "4       3  0.617882      0      0 -0.545934        True   True         1   \n",
      "5       3 -0.005046      0      0 -0.513517        True   True         1   \n",
      "\n",
      "   embarked_Q  embarked_S  ...  who_woman  deck_B  deck_C  deck_D  deck_E  \\\n",
      "0           0           1  ...          0       0       0       0       0   \n",
      "2           0           1  ...          1       0       0       0       0   \n",
      "3           0           1  ...          1       0       1       0       0   \n",
      "4           0           1  ...          0       0       0       0       0   \n",
      "5           1           0  ...          0       0       0       0       0   \n",
      "\n",
      "   deck_F  deck_G  embark_town_Queenstown  embark_town_Southampton  alive_yes  \n",
      "0       0       0                       0                        1          0  \n",
      "2       0       0                       0                        1          1  \n",
      "3       0       0                       0                        1          1  \n",
      "4       0       0                       0                        1          0  \n",
      "5       0       0                       1                        0          0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical variables to dummy variables\n",
    "X = df_filtered.drop('survived', axis=1)\n",
    "y = df_filtered['survived']\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print(\"Dataset after converting categorical variables to dummy variables:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5181d5e-707e-41e9-acea-909fb888f3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (461, 23)\n",
      "Test set shape: (116, 23)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083959b4-9060-4f2a-9a68-17829fb37feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5e065-7e68-454a-8bc1-c63bf6f8bfac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa31fe-0b02-456f-8b6f-9618dafcd1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
